---
title: "Homework 4"
author: "Halenur Kocaman"
date: "26 Ocak 2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)


```

##Libraries

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(dplyr)
library(glmnet)
library(rattle)
library(rpart.plot)
library(randomForest)
library(data.table)
library(mltools)
library(gbm)
library(e1071)
library(PRROC)
library(Metrics)
library(caret)
```

##Reading The Data

Aim of the regression task is to predict the critical temperature in the superconductivity data set. 81 features exist in the data. All of them are numeric. 

```{r}
reg_data<-read.csv("C:/Users/asus/Desktop/conductivity.csv")
reg_data = reg_data[1:2000,]
str(reg_data)
target_name = "critical_temp"
```

##Train & Test Split

```{r}
size_reg<-floor(0.75*nrow(reg_data))
set.seed(1)
train_ind_reg <- sample(seq_len(nrow(reg_data)),size = size_reg)
reg_train<-reg_data[train_ind_reg,]

reg_test<-reg_data[-train_ind_reg,]

sum(is.na(reg_data)) #0
x_train = model.matrix(critical_temp~., reg_train)[,-1] # trim off the first column
                                         # leaving only the predictors
x_train = x_train[,1:50]
y_train = reg_train %>%
  select(critical_temp) %>%
  unlist() %>%
  as.numeric()

x_test = model.matrix(critical_temp~., reg_test)[,-1] # trim off the first column
                                         # leaving only the predictors
x_test= x_test[,1:50]
y_test = reg_test %>%
  select(critical_temp) %>%
  unlist() %>%
  as.numeric()
```


##Penalized Regression

In lasso regression optimal lambda value is found according to the cross validation. Corresponding lambda min value is 0.001. Train and test prediction performances are measured by rmse values. Train predictions have 18.64 as root mean squared error, while test predictions have 18.72 MSE.

```{r}

cvfit_reg<-cv.glmnet(x_train,y_train,family='gaussian',type.measure='mse')

plot(cvfit_reg)
cvfit_reg$lambda.min #0.001

#train prediction

glm_train_predict<-predict(cvfit_reg,x_train,s='lambda.min')
head(glm_train_predict)
RMSE(glm_train_predict,y_train) #rmse 18.64 for train 

#test prediction
glm_test_predict<-predict(cvfit_reg,x_test,s='lambda.min')
RMSE(glm_test_predict,y_test) #rmse 18.72 for test
```

##Decision Tree

To find the best cp and minbucket, grid is prepared using the values below.

```{r}
tune_control=trainControl(
  method = "cv", # cross-validation
  number = 3 # with n folds 

)
tune.gridcart <- expand.grid(
  cp = c(0.0025,0.005,0.01,0.02)
  )
```

###Model Performance

Test and the train performance of the decision tree models for given parameters can be found below. According to the resulting rmse values, minbucket=2 and cp=0.0025 gives the minimum train error which is 11.76. However, the test error is larger which is 15.97.

```{r}
for ( minbucket_value in c(2,25,50,100)){
  print(minbucket_value)
  model <- train(
    x_train, y_train,method = "rpart",
    trControl = tune_control,
    tuneGrid = tune.gridcart,
    metric= "RMSE",
    control = rpart.control( minbucket = minbucket_value),
    
  )
  
  
  print(model$bestTune$cp)
  train_rf_pred = predict(model,x_train)
  test_rf_pred = predict(model,x_test)
  print(RMSE(train_rf_pred,y_train))
   print(RMSE(test_rf_pred,y_test))
}
```


### Random Forest

The best feature number is selected as 4 in the given set. Corresponding train error for the model is 6.77 while test error is 12.40.

```{r}
tunegrid_rf <- expand.grid(.mtry = c(4,7,10))

model_rf <- train(
    x_train,y_train , method = "rf",
    trControl = tune_control,
    tuneGrid = tunegrid_rf,
    metric= "RMSE",
    control = rpart.control( ntree = 500,minbucket = 5)
    
  )
print(model_rf$bestTune)
plot(model_rf)
pred_rf_train = predict(model_rf,x_train)
pred_rf_test = predict(model_rf,x_test)
print(RMSE(pred_rf_train,y_train))
print(RMSE(pred_rf_test,y_test))

```

### Gradient Boosting



```{r message=FALSE, warning=FALSE}
tune_xgboost= expand.grid(
  nrounds = seq(from = 50, to = 200, by = 50),
  eta = c(0.005, 0.01,0.025,0.4),
  max_depth = c(1,2,3),
  gamma = 0,
  colsample_bytree = 1,
  min_child_weight = 1,
  subsample = 1
)
model_xgb <- train(
    x_train,y_train, method = "xgbTree",
    trControl = tune_control,
    tuneGrid = tune_xgboost,
    metric= "RMSE",
    control = rpart.control(minbucket = 10)
    
  )

plot(model_xgb)
print(model_xgb$bestTune)
pred_xgb_train = predict(model_xgb,x_train)
pred_xgb_test = predict(model_xgb,x_test)
print(RMSE(pred_xgb_train,y_train))
print(RMSE(pred_xgb_test,y_test))

```




