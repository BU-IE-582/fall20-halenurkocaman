---
title: "Homework 4"
author: "Halenur Kocaman"
date: "26 Ocak 2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##Libraries

The necessary libraries for the homework can be found below.

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(dplyr)
library(glmnet)
library(rattle)
library(rpart.plot)
library(randomForest)
library(data.table)
library(mltools)
library(gbm)
library(e1071)
library(PRROC)
library(caret)
```

##Reading The Data

The data is Telco's customer churn data. It has 20 features and this is an example of unbalanced dataset. There are many information about the customer. The aim is to understand whether the customer will churn or not. Target variable has two alternatives as yes and no. Churned customers consist of approximately 1/3 of the dataset. Features are mostly categorical and there are also some numeric features such as monthly and total charges.

```{r}
churn_df<-read.csv("/Users/asus/Desktop/churn.csv")
churn_df[sapply(churn_df, is.character)] <- lapply(churn_df[sapply(churn_df, is.character)],as.factor)
churn_df = churn_df[,-1]
str(churn_df)
summary(churn_df)
```

##Train & Test Split

The data is split into test and train data using the ratio 0.75-0.25.

```{r}
size_churn<-floor(0.75*nrow(churn_df))
set.seed(1)
train_ind_churn <- sample(seq_len(nrow(churn_df)),size = size_churn)
#churn train contains the target
churn_train<-churn_df[train_ind_churn,]
churn_test<-churn_df[-train_ind_churn,]
churn_train <- na.omit(churn_train)
churn_test<-na.omit(churn_test)
#one hot encoded, as matrix, but target is eliminated
churn_train_one_hot<-one_hot(as.data.table(churn_train[,-20])) # remove target
churn_train_one_hot<-as.matrix(churn_train_one_hot)
churn_test_one_hot<-one_hot(as.data.table(churn_test[,-20]))
churn_test_one_hot<-as.matrix(churn_test_one_hot)

```

##ROC Curve Function

ROC curve will be used to evaluate the performance of the model. Accuracy is not used in this classification problem because the dataset is imbalanced. Therefore if accuracy was used, model would be biased towards "No Churn" type customers.

```{r}
calc_auprc_logistic <- function( data,predictions){
  
  index_class2 <- data$Churn == "Yes"
  index_class1 <- data$Churn == "No"
  
  
  
  pr=pr.curve(predictions$No[index_class2], predictions$Yes[index_class1], curve = TRUE)
  roc = roc.curve(predictions$No[index_class2], predictions$Yes[index_class1], curve = TRUE)
  plot(pr)
  plot(roc)
}

calc_auprc <- function(model,data){
  
  index_class2 <- data$Churn == "Yes"
  index_class1 <- data$Churn == "No"
  
  predictions <- predict(model, data, type = "prob")
  
  
  roc = roc.curve(predictions$No[index_class2], predictions$Yes[index_class1], curve = TRUE)
  
  plot(roc)
  return(roc)
}
```

##Logistic Regression

Firstly logistic regression is used to perform churn prediction. Looking at the lambda values in the graph, lambda min is used as the optimal lambda and it is equal to 0.000265. It uses 29 nonzero features to perform logistic regression.

```{r}
cvfit_churn<-cv.glmnet(churn_train_one_hot,churn_train$Churn,family='binomial',type.measure='auc')
cvfit_churn
plot(cvfit_churn)
coef(cvfit_churn,s="lambda.min")

```

In train prediction, AUC is found as 0.871. In test prediction AUC value is 0.869. They are very close to each other. Hence, we could not derive any results of underfitting or overfitting.   

```{r}
#train prediction
predicted_glm_churn_train<-predict(cvfit_churn,churn_train_one_hot,s='lambda.min',type='response')
head(predicted_glm_churn_train)
prob<-data.frame("No"=(1-predicted_glm_churn_train),"Yes"=predicted_glm_churn_train)
colnames(prob)=c("No","Yes")
calc_auprc_logistic(churn_train,prob)

```

```{r}
#test prediction
predicted_glm_churn_test<-predict(cvfit_churn,churn_test_one_hot,s='lambda.min',type='response')
prob_test<-data.frame("No"=(1-predicted_glm_churn_test),"Yes"=predicted_glm_churn_test)
colnames(prob_test)=c("No","Yes")
calc_auprc_logistic(churn_test,prob_test)

```


##Decision Tree

Secondly, decision tree is performe in the churn dataset. In decision tree setting, two parameters need to be tuned for this task. For compexity parameter 0.005,0.001,0.0015,0.002 are selected. For minbucket value 10,15 and 20 are selected. 5 fold cross-validation is performed to find the best model.

```{r}
tune_control=trainControl(
  method = "cv", # cross-validation
  number = 5, # with n folds 
  summaryFunction = twoClassSummary,
  classProbs = TRUE
)
tune.gridcart <- expand.grid(
  cp = c(0.001,0.0025,0.005,0.01)
  )
```

###Model Performance
For each minbucket value complexity parameter equals to 0.01. The parameters giving the maximum AUC value are cp=0.001 and minbucket=20. When the test and train performances are compared, there is no sign of overfitting or underfitting. Test AUC score is 0.8574

```{r}
for ( minbucket_value in c(10,15,20)){
  print(minbucket_value)
  model <- train(
    Churn ~., data = churn_train, method = "rpart",
    trControl = tune_control,
    tuneGrid = tune.gridcart,
    metric= "ROC",
    control = rpart.control( minbucket = minbucket_value),
    
  )
  roc_value_train = calc_auprc(model,churn_train)
  roc_value_test = calc_auprc(model,churn_test)
  print(plot(model))
  print(model$bestTune) #best cp
  print(roc_value_train$auc)
  print(roc_value_test$auc)
  
}
```


### Random Forest

In this task only the number of features selected for each split is tuned as parameter.4,7 and 10 features are tried as tuning parameters. Number of trees and minbucket values are taken as 500 and 5 respectively. According to the result of the parameter tuning, 4 features selected randomly gives the best AUC value. There might be overfitting in the data because the AUC value in the test data is 2% smaller than the train prediction. Test prediction AUC value is 0.850.

```{r}
tunegrid_rf <- expand.grid(.mtry = c(4,7,10))

model_rf <- train(
    Churn ~., data = churn_train, method = "rf",
    trControl = tune_control,
    tuneGrid = tunegrid_rf,
    metric= "ROC",
    control = rpart.control( ntree = 500,minbucket = 5),
    
  )
model_rf$bestTune
roc_value_rf_test = calc_auprc(model_rf,churn_test)
roc_value_rf_train = calc_auprc(model_rf,churn_train)
plot(model_rf)
```

### Gradient Boosting

In gradient boosting model depth, learning rate and number of trees are the parameters to tune. As number of trees 200,250,300,350,400 are used. For learning rate, 0.005, 0.01,0.025 and 0.4 values are selected. Lastly, model depth is selected as 1,2 and 4. According to the grid defined below, model is trained.AUC value for the test prediction is 0.879. As number of iterations and depth of the trees are increase, improvement in the learning rate decreases the cross validation performance. As an example, for eta= 0.4, there is a line with a negative slope in the figures. However, for eta=0.1 the line has a positive slope.
As a result of the four models gradient boosting is the best performer in the test set with 0.879 AUC value.


```{r message=FALSE, warning=FALSE}
tune_xgboost= expand.grid(
  nrounds = seq(from = 200, to = 400, by = 50),
  eta = c(0.005, 0.01,0.025,0.4),
  max_depth = c(1,2,4),
  gamma = 0,
  colsample_bytree = 1,
  min_child_weight = 1,
  subsample = 1
)
model_xgb <- train(
    Churn ~., data =churn_train, method = "xgbTree",
    trControl = tune_control,
    tuneGrid = tune_xgboost, control=rpart.control(minbucket=10),
    metric= "ROC"
  )

roc_value_xgb_train = calc_auprc(model_xgb,churn_train)
roc_value_xgb_test = calc_auprc(model_xgb,churn_test)
plot(model_xgb)
```




